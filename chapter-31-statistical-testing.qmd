```{r echo = FALSE, warning = FALSE, message=FALSE}
source("init.R")
source("images/part_3/part_3_theory_testing.R")

```

# Statistical testing {#sec-temp}

*Last modified on `r format(fs::file_info("chapter-31-statistical-testing.qmd")$modification_time, '%d. %B %Y at %H:%M:%S')`*

> *"A quote." --- Dan Meyer*

## General background


> *"Statistik ist: Wenn der Jäger am Hasen einmal links und einmal rechts vorbeischießt, dann ist der Hase im Durchschnitt tot." --- Mike Krüger, German comedian*

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 5.5
#| fig-width: 12
#| fig-cap: "foo"
#| label: fig-bias-variance

geom_bulls_eye <- function(x, y) {
  list(
    stat_circle(aes(x0 = x, y0 = y, r = 0.2), fill = "#E16462FF", color = "gray50"),
    stat_circle(aes(x0 = x, y0 = y, r = 0.5), color = "gray50"),
    stat_circle(aes(x0 = x, y0 = y, r = 1), color = "gray50"),
    stat_circle(aes(x0 = x, y0 = y, r = 1.5), color = "gray50"),
    stat_circle(aes(x0 = x, y0 = y, r = 2), color = "gray50"),
    stat_circle(aes(x0 = x, y0 = y, r = 2.5), color = "gray50"),
    stat_circle(aes(x0 = x, y0 = y, r = 3), color = "gray50"),
    stat_circle(aes(x0 = x, y0 = y, r = 3.5), color = "gray50")
  )
}

error_tbl <- tibble(x = seq(0, 3, by = 0.1),
                    y_1 = x^2+1,
                    y_2 = exp(-x+2.65),
                    y_sum = y_1 + y_2)
min_y_sum_tbl <- error_tbl |> 
  mutate(fct = seq(1, 29/17, length.out = 31)) |> 
  filter(y_sum == min(y_sum))

ggplot() +
  theme_minimal() +
  coord_cartesian(xlim = c(-9.1, 30), ylim = c(-9.2, 9.1)) +
  scale_x_continuous(breaks = seq(-10,30,1)) +
  scale_y_continuous(breaks = seq(-10,10,1)) +
  #  geom_hline(yintercept = c(-10, -2, 6), color = "gray75") +
  #  geom_vline(xintercept = c(-6, 2, 10), color = "gray75") +
  ## bulls eye
  geom_bulls_eye(x = -2, y = 2) +  
  geom_bulls_eye(x = 6, y = 2) +  
  geom_bulls_eye(x = -2, y = -6) +  
  geom_bulls_eye(x = 6, y = -6) +  
  geom_point(data = tibble(x = rnorm(12, -3, 1.25),
                           y = rnorm(12, 5, 1.25)), 
             aes(x,y), shape = 19) +
  geom_point(data = tibble(x = rnorm(10, 4, 0.25),
                           y = rnorm(10, 4, 0.25)), 
             aes(x,y), shape = 19) +
  geom_point(data = tibble(x = rnorm(10, 6, 0.2),
                           y = rnorm(10, -6, 0.2)), 
             aes(x,y), shape = 19) +
  geom_point(data = tibble(x = rnorm(12, -2, 1),
                           y = rnorm(12, -6, 1)), 
             aes(x,y), shape = 19) +
  ## coords
  annotate("segment", x = -10, y = -10, xend = 10, yend = -10, size = 1) +
  annotate("segment", x = -8, y = -2, xend = 10, yend = -2, size = 1) +
  annotate("segment", x = -10, y = 6, xend = 10, yend = 6, size = 1) +
  annotate("segment", x = -6, y = 10, xend = -6, yend = -10, size = 1) +
  annotate("segment", x = 2, y = 8, xend = 2, yend = -10, size = 1) +
  annotate("segment", x = 10, y = 10, xend = 10, yend = -10, size = 1) +
  ## text
  ## columns
  annotate("text", x = 2, y = 9, label = "Variance", fontface = 2, size = 7.5) +
  annotate("text", x = -2, y = 7, label = "High", fontface = 2, size = 5.5) +
  annotate("text", x = 6, y = 7, label = "Low", fontface = 2, size = 5.5) +
  ## rows
  annotate("text", x = -9, y = -2, label = "Bias", fontface = 2, 
           size = 7.5, angle = 90) +
  annotate("text", x = -7, y = 2, label = "High", fontface = 2, size = 5.5, angle = 90) +
  annotate("text", x = -7, y = -6, label = "Low", fontface = 2, size = 5.5, angle = 90) +
  # right
  ## coords
  annotate("text", x = 21.5, y = -9, label = "Complexity of the model", fontface = 2, size = 7.5) +  
  annotate("text", x = 13, y = 0.5, label = "Error", fontface = 2, 
           size = 7.5, angle = 90) +
  geom_line(data = error_tbl,
            aes(x = (x+14)*seq(1, 29/17, length.out = 31), 
                y = (y_1-8)*seq(1, 1.5, length.out = 31)), color = "#0D0887FF", size = 1.5) + 
  geom_line(data = error_tbl,
            aes(x = (x+14)*seq(1, 29/17, length.out = 31), 
                y = (y_2-8)), color = "#B12A90FF", size = 1.5) +
  geom_line(data = error_tbl,
            aes(x = (x+14)*seq(1, 29/17, length.out = 31), 
                y = (y_sum-8)), color = "#FCA636FF", size = 1.5) +
  geom_segment(data = min_y_sum_tbl, aes(x = (x+14)*fct, xend = (x+14)*fct,
                                         y = -8, yend = 9)) +
  annotate("segment", x = 14, y = -8, xend = 29, yend = -8, size = 1,
           arrow = arrow(length = unit(0.03, "npc"), type = "closed")) +
  annotate("segment", x = 14, y = -8, xend = 14, yend = 9, size = 1,
           arrow = arrow(length = unit(0.03, "npc"), type = "closed")) 

```

```{r}
tibble(x = seq(0, 3, by = 0.1),
       y_1 = 0.1*x^2+1,
       y_2 = exp(-x+0.5))
```


```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 5.5
#| fig-width: 7
#| fig-cap: "foo"
#| label: fig-stat-fire-alarm
p_fire_alarm
```



```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 5
#| fig-width: 7
#| fig-cap: "foo"
#| label: fig-test-theory-pval-fisher
p_fisher_newman
```



```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "foo"
#| label: fig-test-theory-dist
p_population_sample
```


```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 5
#| fig-width: 10
#| fig-cap: "foo"
#| label: fig-test-theory-random
p_theory_random
```



```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 4.75
#| fig-width: 10
#| fig-cap: "foo"
#| label: fig-test-theory-upper
p_theory_random_upper
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 10
#| fig-width: 10
#| fig-cap: "foo"
#| label: fig-test-theory-full
p_theory_random_full
```

## Theoretical background

### Fisher's approach: The ‘significance test’

Fisher saw statistics as a tool for inductive reasoning (learning from data for science).

-   Only ONE hypothesis: There is only the null hypothesis ($H_0$ e.g., ‘no effect’). An alternative does not formally exist.
-   The measure (p-value): The p-value is a continuous measure of the strength of evidence against the null hypothesis $H_0$.
    -   $p = 0.01$ Strong evidence against the null hypothesis.
    -   $p = 0.20$ No evidence against the null hypothesis. 
-   The result: One rejects the null hypothesis $H_0$ or one does not make a judgement. One never ‘accepts’ the null hypothesis (one simply has not found enough evidence to reject it).
-   Objective: Gain knowledge through individual experiments.

### Neyman-Pearson's approach: The ‘hypothesis test’

Neyman and Pearson sharply criticised Fisher.
They said, ‘You can't reject anything if you don't know what to accept instead.’ They saw statistics as a decision-making process (behaviourism).

-   TWO hypotheses: There is the null hypothesis ($H_0$) AND a specific alternative hypothesis ($H_A$).
-   Type 1 and 2 errors: Before the experiment begins, the following is determined:
    -   $\alpha$ (alpha): How often am I allowed to incorrectly find an effect? (e.g. 5%)
    -   $\beta$ (Beta): How often am I allowed to mistakenly overlook a real effect? (Power/test strength).
-   The result: A tough decision. ‘Accept $H_0$’ or ‘Reject $H_0$’ (or Accept $H_A$).
-   Goal: Minimisation of losses over many repeated experiments (as in industrial production).

Neymans philosophy: We are not looking for the ‘truth’ in individual cases, but rather we behave in such a way that we are wrong as rarely as possible in 1000 decisions.

### Today's ‘hybrid chaos’

Modern textbooks and software (such as SPSS or R) often use a hybrid that historically makes no sense:

-   We define $\alpha = 5\%$ (Neyman-Pearson).
-   We calculate an exact p-value (Fisher).
-   We report the p-value as evidence (Fisher), but use it for a hard yes/no decision (Neyman-Pearson).
-   We talk about ‘power’ (Neyman-Pearson), but often only test against a non-specific alternative.

This mishmash often leads to misunderstandings, such as that a $p = 0.001$ indicates a ‘stronger effect’ than $p = 0.049$  (Fisher thinking), even though in Neyman-Pearson logic at $\alpha = 5\%$ , one would have to make exactly the same decision in both cases (‘Reject $H_0$’).

## R packages used

## Data

## Alternatives

Further tutorials and R packages on XXX

## Glossary

term

:   what does it mean.

## The meaning of "Models of Reality" in this chapter.

-   itemize with max. 5-6 words

## Summary

## References {.unnumbered}
